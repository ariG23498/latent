<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aakash Kumar Nain (@A_K_Nain)">
<meta name="dcterms.date" content="2022-09-02">

<title>The Latent: Code the Maths - A deeper dive into DDPMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">The Latent: Code the Maths</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/magic-with-latents"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html">Blog</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../archive.html">Archive</a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="button" data-bs-toggle="dropdown" aria-expanded="false">Resources</a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/annotated_research_papers">
 <span class="dropdown-text">Annotated Research Papers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/TF_JAX_tutorials">
 <span class="dropdown-text">TF-JAX Tutorials</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/sayakpaul/vision-transformers-tf">
 <span class="dropdown-text">Vision Transformers in TF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://rish-16.github.io/posts/gnn-math/">
 <span class="dropdown-text">Graph Neural Networks</span></a>
  </li>  
    </ul>
  </li>
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-forward-process" id="toc-the-forward-process" class="nav-link active" data-scroll-target="#the-forward-process">1. The Forward Process</a></li>
  <li><a href="#reparameterization" id="toc-reparameterization" class="nav-link" data-scroll-target="#reparameterization">2. Reparameterization</a></li>
  <li><a href="#the-reverse-process" id="toc-the-reverse-process" class="nav-link" data-scroll-target="#the-reverse-process">3. The Reverse Process</a></li>
  <li><a href="#the-training-objective" id="toc-the-training-objective" class="nav-link" data-scroll-target="#the-training-objective">4. The Training Objective</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">A deeper dive into DDPMs</h1>
<p class="subtitle lead">DDPMs - Part 3</p>
  <div class="quarto-categories">
    <div class="quarto-category">diffusion models</div>
    <div class="quarto-category">generative modelling</div>
    <div class="quarto-category">deep learning</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Aakash Kumar Nain (<a href="https:twitter.com/A_K_Nain"><span class="citation" data-cites="A_K_Nain">@A_K_Nain</span></a>) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 2, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<details>
<summary>
<b>Show the code</b>
</summary>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1234</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"ggplot"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>In the last <a href="https://github.com/AakashKumarNain/diffusion_models/blob/main/notebooks/all_you_need_to_know_about_gaussian.ipynb">notebook</a>, we discussed about Gaussian Distribution, its applications in context of diffusion models, and the forward process in diffusion models. Let’s revisit the forward process equation and the corresponding code that we saw in the last notebook.</p>
<section id="the-forward-process" class="level1">
<h1>1. The Forward Process</h1>
<p><span class="math display">\[
q(x_{1:T}\vert x_{0})
:= \prod_{t=1}^{T}q(x_{t}\vert x_{t-1})
:=\prod_{t=1}^{T}\mathcal{N}(x_{t};\sqrt{1-\beta_{t}} x_{t-1},\ \beta_{t}\bf I) \tag{1}
\]</span></p>
<p>As a refresher, <span class="math inline">\(q(x_{0:T})\)</span> is known as the <strong>forward distribution</strong> and <span class="math inline">\(q(x_{t}\vert x_{t-1}\)</span> is referred as <strong>forward diffusion kernel</strong></p>
<details>
<summary>
<b>Show the code</b>
</summary>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_process_ddpms(img_t_minus_1, beta, t):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Implements the forward process of a DDPM model.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">        img_t_minus_1: Image at the previous timestep (t - 1)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        beta: Scheduled Variance</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        t: Current timestep</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Image obtained at current timestep</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Obtain beta_t. Reshape it to have the same number of</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dimensions as our image array</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    beta_t <span class="op">=</span> beta[t].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate mean and variance</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.sqrt((<span class="fl">1.0</span> <span class="op">-</span> beta_t)) <span class="op">*</span> img_t_minus_1</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> np.sqrt(beta_t)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Obtain image at timestep t using equation (15)</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    img_t <span class="op">=</span> mu <span class="op">+</span> sigma <span class="op">*</span> np.random.randn(<span class="op">*</span>img_t_minus_1.shape)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img_t</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's check if ourforward process function is</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># doing what it is supposed to do on a sample image</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Load image using PIL (or any other library that you prefer)</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"../images/cat.jpg"</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Resize the image to desired dimensions</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>IMG_SIZE <span class="op">=</span> (<span class="dv">128</span>, <span class="dv">128</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> img.resize(size<span class="op">=</span>IMG_SIZE)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Define number of timesteps</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>timesteps <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Generate beta (variance schedule)</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>beta_start <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>beta_end <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.linspace(beta_start, beta_end, num<span class="op">=</span>timesteps, dtype<span class="op">=</span>np.float32)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>processed_images <span class="op">=</span> []</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>img_t <span class="op">=</span> np.asarray(img.copy(), dtype<span class="op">=</span>np.float32) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Run the forward process to obtain img after t timesteps</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(timesteps):</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    img_t <span class="op">=</span> forward_process_ddpms(img_t_minus_1<span class="op">=</span>img_t, beta<span class="op">=</span>beta, t<span class="op">=</span>t)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t<span class="op">%</span><span class="dv">20</span><span class="op">==</span><span class="dv">0</span> <span class="kw">or</span> t<span class="op">==</span>timesteps <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> (img_t.clip(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">255.0</span>).astype(np.uint8)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>        processed_images.append(sample)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Plot and see samples at different timesteps</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(processed_images), figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(processed_images):</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    ax[i].imshow(sample)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    ax[i].set_title(<span class="ss">f"Timestep: </span><span class="sc">{</span>i<span class="op">*</span><span class="dv">20</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    ax[i].axis(<span class="st">"off"</span>)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    ax[i].grid(<span class="va">False</span>)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">"Forward process in DDPMs"</span>, y<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p><img src="./forward_process.png" width="800" height="180"><br></p>
<p>Can you spot a major problem with the above equation? (Hint: Check the loop)</p>
<p>Don’t worry if you didn’t get it. Look at the above code closely. Forget everything from the modelling perspective except for the forward pass. You will notice that to obtain a noisy sample, say at timestep <code>t</code>, we need to iterate from <code>t0</code> to <code>t-1</code>. Why? Because the sample obtained at each timestep is conditioned on the samples from the previous timesteps.</p>
<p>That’s not efficient. What if there are 1000 steps and you want to sample the <code>999th</code> timestep? You will iterate the whole loop, simulating the entire Markov Chain. Now that we know the problem, we should think about how we can do better.</p>
</section>
<section id="reparameterization" class="level1">
<h1>2. Reparameterization</h1>
<p>We know that sum of the independent Gaussians is still a Gaussian. We can leverage this fact to sample from an arbitrary forward step. All we need to do is apply the <code>reparameterization</code> trick.</p>
<p>Let &nbsp; <span class="math inline">\(\alpha_{t} = 1 - \beta_{t},\)</span> &nbsp; and &nbsp; <span class="math inline">\(\bar{\alpha}_{t} = \prod_{i=1}^T \alpha_{i}\)</span></p>
<p>From equation (1) we know that:</p>
<p><span class="math display">\[
q(x_{1:T}\vert x_{0}) := \prod_{t=1}^{T}q(x_{t}\vert x_{t-1}) :=\prod_{t=1}^{T}\mathcal{N}(x_{t};\sqrt{1-\beta_{t}} x_{t-1},\ \beta_{t} \bf{I}) \\
\]</span></p>
<p><span class="math display">\[
\text{or} \ \ q(x_{t}\vert x_{t-1}) = \mathcal{N}(x_{t};\sqrt{1-\beta_{t}} x_{t-1},\ \beta_{t} \bf{I})
\]</span></p>
<p>We can obtain the sample at timestep <code>t</code> as:</p>
<p><span class="math display">\[
x_{t} = \sqrt{1 - \beta_{t}} x_{t-1} +  \sqrt{\beta_{t}}\epsilon_{t-1}; \ \ \text where \ \  \epsilon_{t-1} \sim \mathcal{N}(0, \bf{I})
\]</span></p>
<p>Replacing <span class="math inline">\(\beta\)</span> with <span class="math inline">\(\alpha\)</span> in the above equation we can obtain sample at timestep <code>t</code> as:</p>
<p><span class="math display">\[
\begin{align*}
x_{t} &amp;= \sqrt{\alpha_{t}} x_{t-1} + \sqrt{1 - \alpha_{t}}\epsilon_{t-1} \\
\Rightarrow x_{t} &amp;=
\sqrt{\alpha_{t}} \ \ \underbrace{(\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}}\epsilon_{t-2})}_{\text{( Expanding } x_{t-1})} +
\sqrt{1 - \alpha_{t}}\epsilon_{t-1} \\ \\
&amp;= \sqrt{\alpha_{t} \alpha_{t-1}} x_{t-2} +
\underbrace{\sqrt{\alpha_{t}(1 - \alpha_{t-1})}\epsilon_{t-2}}_{\text{RV1}} +
\underbrace{\sqrt{1 - \alpha_{t}}\epsilon_{t-1}}_{\text{RV2}} \\
\end{align*}
\]</span></p>
<p>The two terms namely RV1, and RV2 on RHS in the above equation are two random variables distributed <strong>normally</strong> with a mean of zero and variances <span class="math inline">\(\alpha_{t}(1 - \alpha_{t-1})\ \)</span>, and <span class="math inline">\((1 - \alpha_{t})\)</span> respectively.</p>
<p>In the last <a href="https://github.com/AakashKumarNain/diffusion_models/blob/main/notebooks/all_you_need_to_know_about_gaussian.ipynb">lesson</a> we learned that if we have two Gaussian distributions with mean values <span class="math inline">\(\mu_{1} , \mu_{2}\)</span> and variances <span class="math inline">\(\sigma_{1}^2 , \sigma_{2}^2\)</span> respectively, then the sum of these two random variables is equivaluent to another random variable with a normal distribution <span class="math inline">\(\mathcal{N}(\mu_{1} + \mu_{2}, \sigma_{1}^2 +\sigma_{2}^2)\)</span>. Applying this to the above equation yields:</p>
<p><span class="math display">\[
\begin{align*}
\Rightarrow x_{t} &amp;=
\sqrt{\alpha_{t} \alpha_{t-1}} x_{t-2} +
\sqrt{\alpha_{t}(1 - \alpha_{t-1}) +
(1 - \alpha_{t})}\bar{z}_{t-2} &amp; \bar{z}_{t-2} \ \text {is the merged Gaussian} \\
&amp;= \sqrt{\alpha_{t} \alpha_{t-1}} x_{t-2} +
\sqrt{1 - \alpha_{t} \alpha_{t-1}}\bar{z}_{t-2} \\
&amp;= \ \ ... \\
\Rightarrow x_{t}&amp;= \sqrt{\bar{\alpha_{t}}} x_{0} +
\sqrt{1 - \bar{\alpha_{t}}}\epsilon \ \  \ \text{ (since } \
\ \bar{\alpha}_{t} = \prod_{i=1}^T \alpha_{i})
\end{align*}
\]</span></p>
<p>From above, we can say that:</p>
<p><span class="math display">\[
q(x_{t}\vert x_{0}) = \mathcal{N}(x_{t};\sqrt{\bar{\alpha_{t}}} x_{0},\ (1 - \bar{\alpha_{t}}) \ \bf{I}) \tag {2}
\]</span></p>
<p>Ha! The above equation is nice.Given the original image, we can now sample at any arbitrary timestep without simulating the entire Markov chain till that step. Before coding it up, let’s recap of what we did and achieve:</p>
<ol type="1">
<li>We figured out that in the original formulation, we need to simulate the Markov chain to the step for which we want to sample.</li>
<li>We reparameterized <span class="math inline">\(\beta\)</span> in terms of <span class="math inline">\(\alpha\)</span></li>
<li>The above reparameterization leads to an equation where we can sample at any arbitrary timestep</li>
</ol>
<p>Let’s code it and compare the results with the previous results</p>
<details>
<summary>
<b>Show the code</b>
</summary>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_process_ddpms_v2(orig_img, alpha_bar, t):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Implements the efficient forward process of a DDPM model.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">        orig_img: Image at timestep t=0</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">        alpha_bar: The reparameterized version of beta</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        t: Current timestep</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Image obtained at current timestep</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Obtain beta_t. Reshape it to have the same number of</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dimensions as our image array</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    alpha_bar_t <span class="op">=</span> alpha_bar[t].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate mean and variance</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.sqrt(alpha_bar_t) <span class="op">*</span> orig_img</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> np.sqrt(<span class="fl">1.0</span> <span class="op">-</span> alpha_bar_t)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Obtain image at timestep t</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    img_t <span class="op">=</span> mu <span class="op">+</span> sigma <span class="op">*</span> np.random.randn(<span class="op">*</span>orig_img.shape)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img_t</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define alpha and alpha_bar</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> beta</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>alpha_bar <span class="op">=</span> np.cumprod(alpha)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>processed_images <span class="op">=</span> [img] <span class="co"># Image at 0th step</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> np.asarray(img.copy(), dtype<span class="op">=</span>np.float32) <span class="op">/</span> <span class="fl">255.</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Run the forward pass for specific timesteps</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># We will use the timesteps we used in previous visualizations</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>specific_timesteps <span class="op">=</span> [<span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">59</span>, <span class="dv">79</span>, <span class="dv">99</span>]</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> specific_timesteps:</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    img_t <span class="op">=</span> forward_process_ddpms_v2(orig_img, alpha_bar, step)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    img_t <span class="op">=</span> (img_t.clip(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">255.0</span>).astype(np.uint8)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    processed_images.append(img_t)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Plot and see samples at different timesteps</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(processed_images), figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(processed_images):</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    ax[i].imshow(sample)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    ax[i].set_title(<span class="ss">f"Timestep: </span><span class="sc">{</span>i<span class="op">*</span><span class="dv">20</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    ax[i].axis(<span class="st">"off"</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    ax[i].grid(<span class="va">False</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">"Efficient Forward process in DDPMs"</span>, y<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"../plots/efficient_forward_process.png"</span>,</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>            pad_inches<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>            bbox_inches<span class="op">=</span><span class="st">'tight'</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>           )</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p><img src="./efficient_forward_process.png" width="800" height="180"><br></p>
<p>Now that we are aware of everything involved in the forward process, we need to figure out how are we going to generate image from the noise we got at the end of the forward process.</p>
</section>
<section id="the-reverse-process" class="level1">
<h1>3. The Reverse Process</h1>
<p>We should end up with a pure noise distribution by the end of the forward process, given we set the variance schedule appropriately i.e.&nbsp;the distribution we will end up with will be <span class="math inline">\(\sim \mathcal{N}(x_{T}; 0,I)\)</span>. For the reverse process, we will start with noise, and will try to undo the noise at each timestep to obtain back the original image. We can write this process as:</p>
<p><span class="math display">\[
p_{\theta}(x_{0:T})
:= p(x_{T}) \prod_{t=1}^T p_{\theta}(x_{t-1} | x_{t})
:= p(x_{T}) \prod_{t=1}^T \mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}, t)) \tag{3}
\]</span></p>
<p>where the parameters of the multivariate Gaussian are time-dependent and are to be learned.</p>
<p>A few things to note:</p>
<ol type="1">
<li><span class="math inline">\(p(x_{0:T})\)</span> is the <strong>reverse distribution</strong> and <span class="math inline">\(p(x_{t-1} | x_{t})\)</span> is known as the <strong>reverse diffusion kernel</strong></li>
<li><span class="math inline">\(\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}\)</span> are the learnable parameters of the reverse distribution</li>
<li>The forward process can be seen as pushing the sample off the data mainfold, turning it into noise. The reverse process can be seen as pushing the sample back to the manifold by removing the noise. (Words taken from Ari Seff’s <a href="https://www.youtube.com/watch?v=fbLgFrlTnGU">tutorial</a> because I don’t think there is any better way to put it.)</li>
<li><span class="math inline">\(p(x_{T})\)</span> is nothing but <span class="math inline">\(q(x_{T})\)</span> i.e.&nbsp;the point where the forward process ends is the starting point of the reverse process.</li>
<li>There can be <code>n</code> number of pathways to arrive at a sample <span class="math inline">\(p_{\theta}(x_{0})\)</span> starting from a noise sample. To obtain <span class="math inline">\(p_{\theta}(x_{0})\)</span>, we would then be required to integrate over all the possible pathways i.e.&nbsp;<span class="math inline">\(p_{\theta}(x_{0}) = \int p_{\theta}(x_{0:T})dx_{1:T}\)</span></li>
<li>Calculating density like in the above step is intractable. A neural network is sufficient to predict the mean <span class="math inline">\(\mu_{\theta}\)</span> and the diagonal covariance matrix <span class="math inline">\(\Sigma_{\theta}\)</span> for the reverse process as shown below in the equation, but we would also be required to frame our objectve function differently</li>
</ol>
</section>
<section id="the-training-objective" class="level1">
<h1>4. The Training Objective</h1>
<p>Let’s write down all the equations we saw for the reverse process, again before we move to the discussion on training objective.</p>
<p><span class="math display">\[
\begin{align*}
p_{\theta}(x_{0}) &amp;= \int p_{\theta}(x_{0:T})dx_{1:T} \tag{4} \\
p_{\theta}(x_{t-1}|x_{t}) &amp;:= \mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}, t)) \tag{5}
\end{align*}
\]</span></p>
<p>If eq.<em>(4)</em> is intractable, then how do we frame our objective function?</p>
<p>If you have worked with other types of generative models before, then you might have seen something like equation eq.<em>(4)</em> in other types of generative models. But where? The answer is <strong>Variational AutoEncoder</strong>(VAE for short). If we treat <span class="math inline">\(x_{0}\)</span> as <strong>observed variable</strong> and <span class="math inline">\(x_{1:T}\)</span> as <strong>latent variables</strong>, then we have a setup similar to a VAE.</p>
<p>Similarities:</p>
<ol type="1">
<li>The forward process can be seen as the equivalent of the encoder in a VAE converting data to latents</li>
<li>The reverse process can be seen as the equivalent of the decoder in a VAE producing data from latents</li>
<li>Given that the above two hold, we can maximize the lower bound (Check out this <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#loss-function-elbo">post</a> if you need a referesher on ELBO)</li>
</ol>
<p>Differences:</p>
<ol type="1">
<li>Unlike a VAE where the encoder is jointly trained with the decoder, the forward process in DDPMs is fixed. Only the reverse part is trainable</li>
<li>Unlike in VAEs, the latents in DDPMs have the same dimensionality as the observed variable</li>
<li>Variation lower bound or Evidence lower bound (ELBO for short), in the case of DDPMs, is a sum of losses at each time step <code>t</code>, <span class="math inline">\(L = L_{0} + L_{1} + ... + L_{T}\)</span> <br></li>
</ol>
<p><img src="vae_ddpm.jpg" class="img-fluid"></p>
<p>First, let’s write down the lower bound we see in a VAE here. Let’s say <strong>x</strong> is an observed variable, and <strong>z</strong> is the latent variable (in context of a VAE). It is known that:</p>
<p><span class="math display">\[
\begin{align*}
&amp;\text{log}\ p_{\theta}(x) \ge \text{variational lower bound} \\
\Rightarrow &amp;\text{log}\ p_{\theta}(x) \ge \mathbb{E}_{q_{\phi}(z|x)}[\text{log}\ p_{\theta}(x|z)] \ - \ D_{KL}(q_{\phi}(z|x) \parallel p_{\theta}(z)) \tag{6}
\end{align*}
\]</span></p>
<p>As discussed earlier, when it comes to diffusion models, we can treat <span class="math inline">\(x_{0}\)</span> as the observed variable, and <span class="math inline">\(x_{1:T}\)</span> as the latent varaibles. Substituting these in equation <em>(6)</em> yields:</p>
<p><span class="math display">\[
\begin{align*}
\text{log}\ p_{\theta}(x_{0}) &amp;\ge \mathbb{E}_{q(x_{1:T}|x_{0})}[\text{log}\ p_{\theta}(x_{0}|x_{1:T})] \ - \ \underbrace{D_{KL}(q(x_{1:T}|x_{0}) \parallel p_{\theta}(x_{1:T}))} \\ \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_{0})}[\text{log}\ p_{\theta}(x_{0}|x_{1:T})] \ - \ \mathbb{E}_{q(x_{1:T}|x_{0})}\big[\text{log}\frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{1:T})}\big] \\
&amp;=\mathbb{E}_{q}[\text{log}\ p_{\theta}(x_{0}|x_{1:T})] \ - \
\mathbb{E}_{q} \big[ \text{log}\frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{1:T})} \big]
\\
&amp;=\mathbb{E}_{q}\bigg[\text{log}\ p_{\theta}(x_{0}|x_{1:T}) \ - \
\text{log}\frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{1:T})}\bigg]
\\
&amp;=\mathbb{E}_{q}\biggl[\text{log}\ p_{\theta}(x_{0}|x_{1:T}) \ + \ \text{log}\frac{p_{\theta}(x_{1:T})}{q(x_{1:T}|x_{0})}\biggr]
\\
&amp;=\mathbb{E}_{q}\biggl[\text{log}\ \frac{p_{\theta}(x_{0}|x_{1:T}) \ p_{\theta}(x_{1:T})}{{q(x_{1:T}|x_{0})}}\biggr]
\\
&amp;=\mathbb{E}_{q}\biggl[\text{log}\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_{0})}\biggr] \tag{7} \\
\end{align*}
\]</span></p>
<p>The maths above looks scary but if you look closely, we haven’t done anything fancy apart from applying standard definitions of expectation, KL-Divergence, and log to the original equation. Let’s denote equation <em>(7)</em> by <span class="math inline">\({L}\)</span></p>
<p><span class="math display">\[
\begin{align*}
{L} &amp;=\mathbb{E}_{q}\biggl[\text{log}\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_{0})}\biggr] \\
&amp;= \mathbb{E}_{q}\biggl[\text{log}\frac{p(x_{T})\prod_{t=1}^T p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^T q(x_{t}|x_{t-1})}\biggr] \\
\Rightarrow{L} &amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \underbrace{\sum\limits_{t=1}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})}}\biggr]
\end{align*}
\]</span></p>
<p>Using Bayes’ rule, we know that, <span class="math display">\[
\begin{align*}
q(x_{t-1} | x_{t}, x_{0}) &amp;= q(x_{t} | x_{t-1}, x_{0}) \frac{q(x_{t-1}| x_{0})}{q(x_{t} | x_{0})} \tag{8}\\
{L} &amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \underbrace{\sum\limits_{t=1}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})}}\biggr] \\
\end{align*}
\]</span></p>
<p>Using (8) we can rewrite this as:</p>
<p><span class="math display">\[\begin{align*}
L &amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \sum\limits_{t=2}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})} \ + \ \text{log}\frac{p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}\biggr] \\

&amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \text{log}\frac{p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})} \ + \ \sum\limits_{t=2}^{T}\text{log}\frac{q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}\ + \ \sum\limits_{t=2}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \\

&amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \text{log}\frac{p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})} \ + \ \underbrace{\sum\limits_{t=2}^{T}\text{log}\bigl(q(x_{t-1}|x_{0})\bigr) \ - \ \text{log}\bigl({q(x_{t}|x_{0})\bigr)}} \ + \ \sum\limits_{t=2}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \\
\end{align*}\]</span></p>
<p>Expanding that summation, we get:</p>
<p><span class="math display">\[
\begin{align*}
L &amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \text{log}\frac{p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})} \ + \ \text{log}\frac{q(x_{1} | x_{0})}{q(x_{T}|x_{0})} \ + \ \sum\limits_{t=2}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \\
&amp;= \mathbb{E}_{q}\biggl[\text{log}\ p(x_{T}) \ + \ \text{log }{p_{\theta}(x_{0}|x_{1})\ - \ \text{log }q(x_{1}|x_{0})} \ + \ \text{log }q(x_{1} | x_{0}) \ - \ \text{log }{q(x_{T}|x_{0})} \ + \ \sum\limits_{t=2}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \\
&amp;= \mathbb{E}_{q(x_{1:T}|x_{0})}\biggl[\text{log} \frac{p(x_{T})}{q(x_{T}|x_{0})} \ + \ \text{log }{p_{\theta}(x_{0}|x_{1})} \ + \ \sum\limits_{t=2}^{T}\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \\
\Rightarrow L &amp;= \mathbb{E}_{q(x_{1}|x_{0})}\big[\text{log }{p_{\theta}(x_{0}|x_{1})}\big] \ + \ \mathbb{E}_{q(x_{T}|x_{0})}\big[\text{log }\frac{p(x_{T})}{q(x_{T}|x_{0})}\big] \ + \  \sum\limits_{t=2}^{T}\mathbb{E}_{q(x_{t},\ x_{t-1}|x_{0})}\bigg[\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \tag{9}\\
\end{align*}
\]</span> <br></p>
<p>In equation <em>(9)</em> we used linearity of expectation to expand the terms and rearranged the subscripts accordingly. This has provided us an opportunity to express the expression <code>L</code> in terms of KL-divergence. Let’s do that</p>
<p><span class="math display">\[
\begin{align*}
L &amp;= \mathbb{E}_{q(x_{1}|x_{0})}\big[\text{log }{p_{\theta}(x_{0}|x_{1})}\big] \ + \ \mathbb{E}_{q(x_{T}|x_{0})}\big[\text{log }\frac{p(x_{T})}{q(x_{T}|x_{0})}\big] \ + \  \sum\limits_{t=2}^{T}\mathbb{E}_{q(x_{t},\ x_{t-1}|x_{0})}\bigg[\text{log}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t}, x_{0})}\biggr] \\
\Rightarrow L &amp;= \mathbb{E}_{q(x_{1}|x_{0})}\big[\text{log }{p_{\theta}(x_{0}|x_{1})}\big] \ - \ D_{KL}\bigg(q(x_{T}|x_{0}) \parallel \ p(x_{T}) \bigg) \ - \ \sum\limits_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\big[D_{KL}\big(q(x_{t-1}|x_{t}, x_{0}) \ \parallel \ p_{\theta}(x_{t-1}|x_{t})\big)\big] \tag{10} \\
\end{align*}
\]</span><br></p>
<p>A few points to note about this equation:</p>
<ol type="1">
<li>Take a look at the second term <span class="math inline">\(D_{KL}\big(q(x_{T}|x_{0}) \parallel \ p(x_{T})\big)\)</span>. The forward process represented by <code>q</code> is fixed. The term <span class="math inline">\(p(x_{T})\)</span> is nothing but the end point of the forward process and the start of the reverse process, hence it is fixed as well. This means that we can safely ignore the second term in the optimization process.</li>
<li>The KL-divergence involved in the third term <span class="math inline">\(D_{KL}\big(q(x_{t-1}|x_{t}, x_{0}) \ \parallel \ p_{\theta}(x_{t-1}|x_{t})\big)\)</span> is known as <em>denoising matching term</em>. We proved earlier that if <span class="math inline">\(x_{0}\)</span> is known, we can show that the intermediate step in the forward process <code>q</code> are Gaussian (we will show it later). The reverse steps in <code>p</code> are also parameterized as Gaussian (check eq. (3)), hence we can say that the KL divergence term at each timestep is a comparison between two Gaussians. The term <span class="math inline">\((q(x_{t-1}|x_{t}, x_{0})\)</span> also serves as the GT since it defines how to denoise a noisy image <span class="math inline">\({x_{t}}\)</span> with access to what the final, completely denoised image <span class="math inline">\(x_{0}\)</span> should be.</li>
<li>As a result of above two, we can rewrite our training objective as: <span class="math display">\[
L = \mathbb{E}_{q(x_{1}|x_{0})}\big[\text{log }{p_{\theta}(x_{0}|x_{1})}\big] \ - \ \sum\limits_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\big[D_{KL}\big(q(x_{t-1}|x_{t}, x_{0}) \ \parallel \ p_{\theta}(x_{t-1}|x_{t})\big)\big] - C \tag{11} \\
\]</span></li>
</ol>
<p><br></p>
<p>Let’s revisit equation (8) again now. We know that:</p>
<p><span class="math display">\[
\begin{align*}
q(x_{t-1} | x_{t}, x_{0}) &amp;= q(x_{t} | x_{t-1}, x_{0}) \frac{q(x_{t-1}| x_{0})}{q(x_{t} | x_{0})}
\end{align*}
\]</span> <br></p>
<p>As proved earlier, we can sample any arbritrary forward step given <span class="math inline">\(x_{0}\)</span> as: <span class="math display">\[
q(x_{t}\vert x_{0}) = \mathcal{N}(x_{t};\sqrt{\bar{\alpha_{t}}} x_{0},\ (1 - \bar{\alpha_{t}}) I)
\]</span> <br></p>
<p>Combining the above two facts, we can say that:</p>
<p><span class="math display">\[
\begin{align*}
q(x_{t-1} | x_{t}, x_{0}) &amp;= q(x_{t} | x_{t-1}, x_{0}) \frac{q(x_{t-1}| x_{0})}{q(x_{t} | x_{0})} \\
&amp;= \frac{\mathcal{N}(x_{t};\sqrt{\bar{\alpha_{t}}} x_{0},\ (1 - \alpha_{t}) \ I) \ \ \mathcal{N}(x_{t-1};\sqrt{\bar{\alpha}_{t-1}} x_{0},\ (1 - \bar{\alpha}_{t-1}) \ I)}{\mathcal{N}(x_{t};\sqrt{\bar{\alpha_{t}}} x_{0},\ (1 - \bar{\alpha_{t}}) \ I)} \\
&amp;\propto \text{exp} \bigg\{-\frac{1}{2}\biggl[
\frac{(x_{t} - \sqrt{\alpha_{t}}x_{t-1})^2}{(1 \ - \ \alpha_{t})} \ + \
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}}x_{0})^2}{(1 \ - \ \bar{\alpha}_{t-1})} \ - \
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t}}x_{0})^2}{(1 \ - \ \bar{\alpha}_{t})}
\biggr]\bigg\} \\
&amp;= \text{exp} \bigg\{-\frac{1}{2}\biggl[
\frac{(x_{t} - \sqrt{\alpha_{t}}x_{t-1})^2}{\beta_{t}} \ + \
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}}x_{0})^2}{(1 \ - \ \bar{\alpha}_{t-1})} \ - \
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t}}x_{0})^2}{(1 \ - \ \bar{\alpha}_{t})}
\biggr]\bigg\} \\
&amp;= \text{exp} \bigg\{-\frac{1}{2}\biggl[
\frac{x_{t}^2 - 2\sqrt{\alpha_{t}}x_{t-1}x_{t} \ + \ \alpha_{t}x_{t-1}^2}{\beta_{t}} \ + \
\frac{x_{t-1}^2 - 2\sqrt{\bar{\alpha}_{t-1}}x_{0}x_{t-1} \ + \ \bar{\alpha}_{t-1}x_{0}^2}{(1 \ - \ \bar{\alpha}_{t-1})} \ - \
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t}}x_{0})^2}{(1 \ - \ \bar{\alpha}_{t})}
\biggr]\bigg\} \\
&amp;= \text{exp} \bigg\{-\frac{1}{2}\biggl[
\bigg(\frac{\alpha_{t}}{\beta_{t}} \ + \ \frac{1}{1 \ - \ \bar{\alpha}_{t-1}}\bigg) x_{t-1}^2 \ - \
\bigg(\frac{2\sqrt{\alpha_{t}}\ x_{t}}{\beta_{t}} \ + \ \frac{2\sqrt{\bar{\alpha}_{t-1}}\ x_{0}}{1 \ - \ \bar{\alpha}_{t-1}}\bigg) x_{t-1} \ + \ C(x_{t}, x_{0})
\biggr]\bigg\} \tag{12} \\
\end{align*}
\]</span> <br></p>
<p>The term <span class="math inline">\(C(x_{t}, x_{0})\)</span> is some value computed using <span class="math inline">\(x_{t}, x_{0}, \text{and} \ \alpha\)</span>, and doesn’t involve <span class="math inline">\(x_{t-1}\)</span>. Hence we treat it as a constant term w.r.t to <span class="math inline">\(x_{t-1}\)</span>. We know that in the case of a standard Gaussian, the density is proportional to:</p>
<p><span class="math display">\[
\begin{align*}
&amp;\propto \text{exp}\big(- \ \frac{(x - \mu)^2}{2\sigma^2}\big) \\
&amp;=\text{exp}\big(- \frac{1}{2} \frac{(x^2 \ + \ \mu^2 \ - \ 2\mu x)}{\sigma^2}\big) \tag{13} \\
\end{align*}
\]</span> <br></p>
<p>Comparing the coefficients of our Gaussian in <strong>(12)</strong> and the standard Gaussian in <strong>(13)</strong> we get:</p>
<p><span class="math display">\[
\begin{align*}
\tilde{\beta_{t}} &amp;=
1 \ \big/ \bigg(\frac{\alpha_{t}}{\beta_{t}} \ + \ \frac{1}{1 \ - \ \bar{\alpha}_{t-1}}\bigg) =
\frac{1- \bar{\alpha}_{t-1}}{1- \bar{\alpha}_t} \beta_{t}
= \frac{(1- \bar{\alpha}_{t-1})(1 - \alpha_{t})}{1 - \bar{\alpha}_{t}} \tag{14} \\ \\ \\
\tilde{\mu}(x_{t}, x_{0}) &amp;=
\frac
{
\dfrac{\sqrt{\alpha_{t}}\ x_{t}}{\beta_{t}} \ + \ \dfrac{\sqrt{\bar{\alpha}_{t-1}}\ x_{0}}{1 \ - \ \bar{\alpha}_{t-1}}
}
{
\dfrac{\alpha_{t}}{\beta_{t}} \ + \ \dfrac{1}{1 \ - \ \bar{\alpha}_{t-1}}
} \\ \\
&amp;=\dfrac{\sqrt{\alpha_{t}}\ x_{t}}{\beta_{t}} \ + \ \dfrac{\sqrt{\bar{\alpha}_{t-1}}\ x_{0}}{1 \ - \ \bar{\alpha}_{t-1}} \\ \\
&amp;=\bigg(
\dfrac{\sqrt{\alpha_{t}}\ x_{t}}{\beta_{t}} \ + \ \dfrac{\sqrt{\bar{\alpha}_{t-1}}\ x_{0}}{1 \ - \ \bar{\alpha}_{t-1}}
\bigg)
\frac{1- \bar{\alpha}_{t-1}}{1- \bar{\alpha}_t} \beta_{t} \\ \\
\Rightarrow \tilde{\mu}(x_{t}, x_{0}) &amp;=
\frac{\sqrt{\alpha_{t}}(1- \bar{\alpha}_{t-1})}{1- \bar{\alpha}_{t}} x_{t} \ + \
\frac{\sqrt{\alpha_{t-1}}\ \beta_{t}}{1- \bar{\alpha}_{t}} x_{0} \tag{15} \\
\end{align*}
\]</span> <br></p>
<p>When we applied the reparameterization trick, we learned that we can write:<br> <span class="math display">\[
x_{t} =  \sqrt{\bar{\alpha_{t}}}x_{0} + \sqrt{1 - \bar{\alpha_{t}}}\epsilon_{t} \\
\]</span></p>
<p>Hence we can express <em>(15)</em> as:</p>
<p><span class="math display">\[
\begin{align*}
\Rightarrow \tilde{\mu}(x_{t}, x_{0}) &amp;=
\frac{\sqrt{\alpha_{t}}(1- \bar{\alpha}_{t-1})}{1- \bar{\alpha}_{t}} x_{t} \ + \
\frac{\sqrt{\alpha_{t-1}}\ \beta_{t}}{1- \bar{\alpha}_{t}}
\frac{1}{\sqrt{\bar{\alpha}}_{t}} (x_{t} \ - \ \sqrt{1 \ - \ \bar{\alpha}_{t}}\epsilon_{t}) \\
\Rightarrow \tilde{\mu}(x_{t}, x_{0}) &amp;=
\frac{1}{\sqrt{\bar{\alpha}}_{t}} \bigg(x_{t} \ - \ \frac{\beta_{t}}{\sqrt{1 \ - \ \bar{\alpha}_{t}}}\epsilon_{t}\bigg)
\tag{16}
\\ \\
\therefore \ &amp;q(x_{t-1} | x_{t}, x_{0}) \sim \mathcal {N}(x_{t-1}; \tilde{\mu}(x_{t}, x_{0}), \ \tilde{\beta}\bf{I})
\end{align*}
\]</span></p>
<p>So, <span class="math inline">\(q(x_{t-1} | x_{t}, x_{0})\)</span> is the true distribution, and <span class="math inline">\(\tilde{\mu}(x_{t}, x_{0}),\text{, and } \tilde{\beta}\)</span> are the true parameters we are trying to approximate. Let’s talk about the other distriution <span class="math inline">\(p_{\theta}(x_{t-1}, x_{t})\)</span>. Our model has to approximate the conditioned probability distributions in the reverse diffusion process:</p>
<p><span class="math display">\[
p_{\theta}(x_{t-1}|x_{t}) := \mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}, t))
\]</span></p>
<p>Though we can try to learn both the mean and the variance of this distribution, the authors of the DDPMs paper found that learning the variance leads to lower quality samples. They decided to keep the variance <span class="math inline">\(\Sigma_{\theta}(x_{t}, t)\)</span> to time specific constants. Hence our network is solely tasked with learning the mean <span class="math inline">\(\mu_{\theta}(x_{t}, t)\)</span>. The true mean that we are trying to approximate is:</p>
<p><span class="math display">\[
\begin{align*}
\tilde{\mu}(x_{t}, x_{0}) &amp;=
\frac{1}{\sqrt{\bar{\alpha}}_{t}} \bigg(x_{t} \ - \ \frac{\beta_{t}}{\sqrt{1 \ - \ \bar{\alpha}_{t}}}\epsilon_{t}\bigg) =
\frac{1}{\sqrt{\bar{\alpha}}_{t}} \bigg(x_{t} \ - \ \frac{1 \ - \ \alpha_{t}}{\sqrt{1 \ - \ \bar{\alpha}_{t}}}\epsilon_{t}\bigg)
\end{align*}
\]</span></p>
<p><span class="math inline">\(x_{t}\)</span> will be available as an input during training. <span class="math inline">\(\alpha_{t}\)</span> is already known to us in advance (check the Reparameterization section), therefore we can reparameterize the Gaussian noise term instead to predict <span class="math inline">\(\epsilon_{t}\)</span> at a given timestep <code>t</code> as:</p>
<p><span class="math display">\[
\begin{align*}
\mu_{\theta}(x_{t}, t) &amp;=
\frac{1}{\sqrt{\bar{\alpha}}_{t}} \bigg(x_{t} \ - \ \frac{1 \ - \ \alpha_{t}}{\sqrt{1 \ - \ \bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t}, t)\bigg) \tag{17}
\end{align*}
\]</span></p>
<p>Let’s recap of all the things we covered in the last section:</p>
<ol type="1">
<li>We showed that <span class="math inline">\(q(x_{t-1} | x_{t}, x_{0})\)</span> is just a Gaussian, and that the KL-divergence at each timestep (for <span class="math inline">\(t &gt; 1\)</span>) between <span class="math inline">\(q(x_{t-1} | x_{t}, x_{0})\)</span> and <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> is KL-divergence between two Gaussian distributions</li>
<li>During training, we have kept the variances of the reverse process to time-specific constants. Our only task is to learn to approximate the mean during training.</li>
<li>With reparameterization, we can instead learn to predict the noise at each timestep.</li>
</ol>
<p><br></p>
<p>Let’s revisit our objectve function now. Writing down the equation here:<br> <span class="math display">\[
L = \mathbb{E}_{q(x_{1}|x_{0})}\big[\text{log }{p_{\theta}(x_{0}|x_{1})}\big] \ - \ \sum\limits_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\big[D_{KL}\big(q(x_{t-1}|x_{t}, x_{0}) \ \parallel \ p_{\theta}(x_{t-1}|x_{t})\big)\big] \ - C \\
\]</span></p>
<p>We know that the KL divergence between two Gaussian distributions can be evaluated in closed form (Check out this excellent <a href="https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/">post</a> for more details). Also, the variance is fixed, and we only need to minimize the difference between the actual mean and the predicted mean.</p>
<p><span class="math display">\[
\begin{align*}
L_{t\sim U \{ 2, T \} } = \mathbb{E}_{q}\big[D_{KL}\big(
\mathcal {N}(x_{t-1}; \tilde{\mu}(x_{t}, x_{0}), \ \tilde{\beta}{I}) \ \parallel \
\mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}, t)\big) \big] \\
\end{align*}
\]</span> <br></p>
<p>Focusing on the KL divergence part:<br> <span class="math display">\[
\begin{align*}
&amp;D_{KL}\big(
\mathcal {N}(x_{t-1}; \tilde{\mu}(x_{t}, x_{0}), \ \tilde{\beta}{I}) \ \parallel \
\mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}, t)\big)
\\
&amp;=
\frac{1}{2} \bigg[\text{log}\frac{\bar{\beta}}{\Sigma_{\theta}(x_{t}, t)} \ - \ d  \ + \
\big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)^{T} \Sigma_{\theta}(x_{t}, t)^{-1} \big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)\big)\bigg]\\
\end{align*}
\]</span></p>
<p>Because variances are kept fixed, hence we can write it as:</p>
<p><span class="math display">\[
\begin{align*}
&amp;=
\frac{1}{2} \bigg[\text{log}\frac{\bar{\beta}}{\bar{\beta}} \ - \ d  \ + \
\text{tr}\big({\bar{\beta}^{-1} {\bar{\beta}}}\big) \ + \
\big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)^{T} \bar{\beta}^{-1} \big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)\big)\bigg]  \ \ \ \ \
\\
&amp;=
\frac{1}{2} \bigg[\text{log}1 \ - \ d  \ + \ d \ + \
\big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)^{T} \bar{\beta}^{-1} \big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)\big)\bigg]
\\
&amp;=\frac{1}{2\bar{\beta}} \bigg[
\big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)^{T} \big(\bar\mu(x_{t}, x_{0}) \ - \  \mu_{\theta}(x_{t}, t)\big)\big)\bigg]
\end{align*}
\]</span> <br></p>
<p>Now that we have expanded the KL divergence, we can rewrite our loss function as: <br></p>
<p><span class="math display">\[
\begin{align*}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
L_{t\sim U \{ 2, T \} } &amp;= \mathbb{E}_{x_{0}, \epsilon, t}\bigg[
\dfrac{1}{2\bar{\beta}}
\norm{\tilde{\mu}_{\theta}(x_{t}, x_{0})\ - \ \mu_{\theta}(x_{t}, t)}^2
\bigg] \\
&amp;=\mathbb{E}_{x_{0}, \epsilon, t}\bigg[
\dfrac{1}{2 \bar{\beta}}
\norm{
\frac{1}{\sqrt{\bar{\alpha}}_{t}} \bigg(x_{t} \ - \
\frac{1 \ - \ \alpha_{t}}{\sqrt{1 \ - \ \bar{\alpha}_{t}}}\epsilon_{t})\bigg) \ - \
\frac{1}{\sqrt{\bar{\alpha}}_{t}} \bigg(x_{t} \ - \
\frac{1 \ - \ \alpha_{t}}{\sqrt{1 \ - \ \bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t}, t)\bigg)
}^2
\bigg] \\
&amp;=\mathbb{E}_{x_{0}, \epsilon, t}\bigg[
\underbrace{\dfrac{(1 \ - \ \alpha_{t})^2}{2 \alpha_{t} (1 \ - \ \alpha_{t})\bar{\beta}}}_{\text{step-specific weighting}}
\norm{\epsilon_t \ - \ \epsilon_{\theta}(x_{t}, t)}^2
\bigg] \tag{18}
\end{align*}
\]</span> <br></p>
<p>A simpler version was also proposed by the authors, where they also discard this step-specific weigthing term. We can rewrite our final loss function as: <br></p>
<p><span class="math display">\[
\begin{align*}
L^{simple} &amp;= \mathbb{E}_{x_{0}, \epsilon, t}\big[
\norm{\epsilon_t \ - \ \epsilon_{\theta}(x_{t}, t)}^2
\big]
\end{align*}
\]</span></p>
<p><br> The final objective function can be written as: <br> <span class="math display">\[
\begin{align*}
L = L^{simple} \  +  \ C \tag{19}
\end{align*}
\]</span></p>
<p>where <strong>C</strong> is a constant not depending on <span class="math inline">\(\theta\)</span></p>
<p>That’s it for now. I hope this notebook was enough to give you a solid understanding of the Diffusion models and the underlying maths. I tried to break down the maths as much as possible. In the next notebook, we will build a diffusion model in TF/Pytorch from scratch.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li><a href="https://blog.evjang.com/2016/08/variational-bayes.html">A Beginner’s Guide to Variational Methods</a></li>
<li><a href="https://lilianweng.github.io/posts/2018-08-12-vae/">From Autoencoder to Beta-VAE</a></li>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></li>
<li><a href="https://www.youtube.com/watch?v=fbLgFrlTnGU">Ari Seff’s tutorial</a></li>
<li><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></li>
<li><a href="https://arxiv.org/pdf/2208.11970.pdf">Understanding Diffusion Models</a></li>
<li><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Assembly AI: Intro to Diffusion Models</a></li>
<li><a href="https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/">KL Divergence between 2 Gaussian Distributions</a></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>